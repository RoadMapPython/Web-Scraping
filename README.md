# Web-Scrapping
## Componentes usados na live,e um readme para mais detalhes e o link da live gravada: YouTube:
###### -Obs: tem um dicion√°rio de palavras no final do documento

=======================|üìöIntrodu√ß√£oüìö|=====================<br />
Web-Scrapping √© o termo para usar um programa para baixar e processar conte√∫do da web.
Por exemplo, o Google executa muitos programas de Web-Scrapping para indexar 
p√°ginas da Web para seu mecanismo de busca.

Alguns conceitos 
* webbrowser: Vem com python e abre um navegador para uma p√°gina espec√≠fica.

* requests: Baixa arquivos e p√°ginas da Web da internet.

* bs4: Parses HTML, o formato em que as p√°ginas da Web est√£o escritas.

* selenium: Lan√ßa e controla um navegador da Web. O m√≥dulo de sel√™nio √© capaz de preencher formul√°rios e simular cliques do mouse neste navegador.

Quero tentar introduzir dois conceitos e aborda-los nessa live que vai ser o "webbrowser" e o "requests"

Web-Scrapping √© basicamente √© eu extrair dados de paginas web, conseguindo acessar exatamente o que desejo 
,so que no meu codigo em python ,facil ne?

==========================|‚öôÔ∏èConstru√ß√£o do codigo‚öôÔ∏è|============================<br />
Vou come√ßar com o webbrowser

import webbrowser

webbrowser.open('https://sistemas.riopomba.ifsudestemg.edu.br/dacc/index.php/roadmap-python-ensino')

Aqui abrimos a pagina web que desejamos via c√≥digo python, util ne? 

======================================================================<br />

======================================================================<br />
======================================================================<br />
Agora iremos realizar a instala√ß√£o de uma outra biblioteca que iremos usar durante a cria√ß√£o do c√≥digo
* Vamos la no windows+R 
* Digita cmd 
* E digite o comando:pip install requests

======================================================================<br />
Vamos importar a biblioteca "requests" e usar o metodo get

import requests

response = requests.get('https://sistemas.riopomba.ifsudestemg.edu.br/dacc/index.php/roadmap-python-ensino')
print('Status do codigo:', response.status_code)

Os c√≥digos de status das respostas HTTP indicam se uma requisi√ß√£o HTTP foi corretamente conclu√≠da.
As respostas s√£o agrupadas em cinco classes:

* Respostas de informa√ß√£o (100-199),
* Respostas de sucesso (200-299),
* Redirecionamentos (300-399)
* Erros do cliente (400-499)
* Erros do servidor (500-599).

======================================================================<br />
import requests

response = requests.get('https://sistemas.riopomba.ifsudestemg.edu.br/dacc/index.php/roadmap-python-ensino')
print('Status do codigo:', response.status_code)

print('Cabe√ßalho')
print(response.headers)

O cabe√ßalho mostra as informa√ß√µes do site , v√°rios atributos utilizados.

======================================================================<br />
import requests

response = requests.get('https://sistemas.riopomba.ifsudestemg.edu.br/dacc/index.php/roadmap-python-ensino')
print('Status do codigo:', response.status_code)

print('Cabe√ßalho')
print(response.headers)

print('\nConteudo')
print(response.content)

O conteudo ele mostra todo o conteudo do site em si , todo o c√≥digo html dele ,
sabe quando voc√™ clica em inspecionar? ent√£o √© isso!

======================================================================<br />
Iremos utilizar o BeaultifulSoup da biblioteca bs4, com isso teremos de instalar 
o pacote.
* Windows+R 
* Digita cmd 
* E digite o comando:pip install bs4

======================================================================<br />
import requests
from bs4 import BeautifulSoup

response = requests.get('https://sistemas.riopomba.ifsudestemg.edu.br/dacc/index.php/roadmap-python-ensino')

conteudo = response.content

site = BeautifulSoup(conteudo, 'html.parser')

print(type(site))

Estamos salvando o conteudo na v√°riavel "conteudo" 
Na v√°riavel "site" que criamos , passamos a variavel "conteudo" para o BeautifulSoup , 
em formato html.
O print foi somente para visualizar qual o tipo do site , n√£o para imprimir a variavel site,
pois se imprimisse iria aparecer muita coisa

Podemos tamb√©m imprimir com 
print(site.prettify())
que mostra o c√≥digo todo html de uma forma mais organizada no padr√£o html

======================================================================<br />
import requests
from bs4 import BeautifulSoup

response = requests.get('https://sistemas.riopomba.ifsudestemg.edu.br/dacc/index.php/roadmap-python-ensino')

conteudo = response.content

site = BeautifulSoup(conteudo, 'html.parser')
corpo= site.find('div',attrs{'class':'span9 internas' })
print(indice.prettify())

aqui iremos colocar dentro da variavel "corpo" uma parte do site que desejamos ver,
utilizamos o find para achar o div com a classe documentFirstHeading.
attrs=atributos
Ele procura o primeiro div com a classe desejada e talvez mais a frente iremos comentar sobre

======================================================================<br />
import requests
from bs4 import BeautifulSoup

response = requests.get('https://sistemas.riopomba.ifsudestemg.edu.br/dacc/index.php/roadmap-python-ensino')

conteudo = response.content

site = BeautifulSoup(conteudo, 'html.parser')
corpo= site.find('div',attrs{'class':'span9 internas' })

indice= corpo.find('a', attrs={'class': 'documentFirstHeading'})
print(indice.text)

aqui iremos procurar o indice do nosso site na variavel "indice", 
e no print mostramos so o texto do indice que √© "√çndice - Roadmap Python"
======================================================================<br />
import requests
from bs4 import BeautifulSoup
import pandas as pd

lista_noticias = []

response = requests.get('https://g1.globo.com/')

content = response.content

site = BeautifulSoup(content, 'html.parser')

# HTML da not√≠cia
noticias = site.findAll('div', attrs={'class': 'feed-post-body'})

for noticia in noticias:
  # T√≠tulo
  titulo = noticia.find('a', attrs={'class': 'feed-post-link'})

  # print(titulo.text)
  # print(titulo['href']) # link da not√≠cia

  # Subt√≠tulo: div class="feed-post-body-resumo"
  subtitulo = noticia.find('div', attrs={'class': 'feed-post-body-resumo'})

  if (subtitulo):
    # print(subtitulo.text)
    lista_noticias.append([titulo.text, subtitulo.text, titulo['href']])
  else:
    lista_noticias.append([titulo.text, '', titulo['href']])


news = pd.DataFrame(lista_noticias, columns=['T√≠tulo', 'Subt√≠tulo', 'Link'])

news.to_excel('noticias.xlsx', index=False)
======================================================================<br />
---Dicion√°rio---<br />
requests<br />
response<br />
get<br />
content<br />
headers<br />
